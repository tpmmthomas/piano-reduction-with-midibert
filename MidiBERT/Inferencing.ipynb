{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450bf21f-0e49-42db-8cea-f66c96041606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-8jc7uidv because the default path (/uac/ascstd/wkwong/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from miditoolkit.midi import parser as mid_parser  \n",
    "from miditoolkit.midi import containers as ct\n",
    "from transformers import BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from MidiBERT.model import MidiBertSeq2Seq\n",
    "from MidiBERT.modelLM import MidiBertSeq2SeqComplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76f3962-12d0-489c-8522-be4384fcfaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 512, 6]) torch.Size([90, 513, 6])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "skyline_max_len = 90\n",
    "hs = 768\n",
    "seq_len = 512\n",
    "token_len = 6\n",
    "\n",
    "e2w, w2e = np.load('dict/CP_program.pkl', allow_pickle=True)\n",
    "X = np.load('data/processed/String.npy', allow_pickle=True)\n",
    "y = np.load('data/processed/String_ans.npy', allow_pickle=True)\n",
    "X, y = torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8598ea1-5d96-4720-b301-6a2a6a0ad1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at ./s2s_decoder_model/ and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_en = BertConfig(\n",
    "    max_position_embeddings=seq_len,\n",
    "    position_embedding_type=\"relative_key_query\",\n",
    "    hidden_size=hs,\n",
    ")\n",
    "config_de = BertConfig(\n",
    "    max_position_embeddings=seq_len,\n",
    "    position_embedding_type=\"relative_key_query\",\n",
    "    hidden_size=hs,\n",
    ")\n",
    "config_de.is_decoder = True\n",
    "config_de.add_cross_attention = True\n",
    "midibert = MidiBertSeq2Seq(config_en, config_de, '', e2w, w2e)\n",
    "\n",
    "model = MidiBertSeq2SeqComplete(midibert).to(device)\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load('result/seq2seq/MidiBert/model_best.ckpt')\n",
    "for key in list(checkpoint[\"state_dict\"].keys()):\n",
    "            # rename the states in checkpoint\n",
    "            checkpoint[\"state_dict\"][key.replace(\"module.\", \"\")] = checkpoint[\n",
    "                \"state_dict\"\n",
    "            ].pop(key)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fb1af4-19a3-48cd-b2d7-f6d76405824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = np.array([midibert.e2w[etype][\"%s <BOS>\" % etype] for etype in midibert.e2w])\n",
    "PAD = np.array([midibert.e2w[etype][\"%s <PAD>\" % etype] for etype in midibert.e2w])\n",
    "EOS = np.array([midibert.e2w[etype][\"%s <EOS>\" % etype] for etype in midibert.e2w])\n",
    "ABS = np.array([midibert.e2w[etype][\"%s <ABS>\" % etype] for etype in midibert.e2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3688845-d868-4cd6-9cfa-04bc4bfe5565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(token):\n",
    "    token = token.reshape((1, seq_len, token_len))\n",
    "    token = token.to(device)\n",
    "    attn_mask_encoder = (\n",
    "        (token[:, :, 0] != midibert.bar_pad_word)\n",
    "        .float()\n",
    "        .to(device)\n",
    "    )  # (batch, seq_len)\n",
    "\n",
    "    outputs = np.array([BOS])\n",
    "    for i in range(seq_len):\n",
    "        decoder_input_ids = np.array([np.vstack((outputs, np.tile(midibert.pad_word_np, (seq_len - 1 - i, 1))))])\n",
    "        # assert decoder_input_ids.shape == (1, seq_len, token_len)\n",
    "        decoder_input_ids = torch.from_numpy(decoder_input_ids).to(device)\n",
    "        attn_mask_decoder = (\n",
    "            (decoder_input_ids[:, :, 0] != midibert.bar_pad_word)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )  # (batch, seq_len)\n",
    "\n",
    "        # tuples of size 6, each element is a tensor with shape: (batch, seq_len, n_tokens)\n",
    "        predicted_word = model(token, decoder_input_ids, attn_mask_encoder, attn_mask_decoder)\n",
    "\n",
    "        # event to word\n",
    "        temp = []\n",
    "        for j, etype in enumerate(midibert.e2w):\n",
    "            o = np.argmax(predicted_word[j].cpu().detach().numpy(), axis=-1)\n",
    "            temp.append(o)\n",
    "        temp = np.stack(temp, axis=-1)[0][i]\n",
    "        \n",
    "        # stop generating when EOS or PAD is generated\n",
    "        is_end = (temp == EOS).all() or (temp == PAD).all()\n",
    "        print(f'Generated {i} notes', end=\"\\n\" if is_end else \"\\r\")\n",
    "        if is_end:\n",
    "            break\n",
    "        outputs = np.vstack((outputs, temp))\n",
    "\n",
    "    outputs = outputs[1:]\n",
    "    last_pos = 999\n",
    "    changed = 0\n",
    "    for i, tk in enumerate(outputs):\n",
    "        if tk[1] >= last_pos and tk[0] == 0:\n",
    "            outputs[i][0] = 1\n",
    "            changed += 1\n",
    "        last_pos = tk[1]\n",
    "    print(f\"Changed {changed} tokens\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1f557f-0922-47d8-bf85-6829710259ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2mid(page, out_path):\n",
    "    # meta data\n",
    "    out = mid_parser.MidiFile()\n",
    "    out.ticks_per_beat = 480\n",
    "\n",
    "    # First Time Signature\n",
    "    ts = int(page[0][5]+2)\n",
    "    last_ts = ts\n",
    "    current_beat = -ts*480\n",
    "    out.time_signature_changes.append(ct.TimeSignature(ts, 4, 0))\n",
    "\n",
    "    for idx, n in enumerate(page):\n",
    "        # Stop if end or padding starts\n",
    "        if (n == EOS).all() or (n == PAD).all():\n",
    "            break\n",
    "\n",
    "        # Time Signature for THIS note\n",
    "        ts = int(page[idx][5]+2)\n",
    "\n",
    "        # Bar moves forward\n",
    "        if n[0] == 0 or (n[:-1] == ABS[:-1]).all():\n",
    "            current_beat += last_ts*480\n",
    "\n",
    "        # Update new Time Signature if any\n",
    "        if ts != last_ts:\n",
    "            last_ts = ts\n",
    "            out.time_signature_changes.append(ct.TimeSignature(ts, 4, current_beat))\n",
    "\n",
    "        # Add THIS note\n",
    "        if (n[:-1] != ABS[:-1]).all():\n",
    "            program = n[4]\n",
    "            if program not in [i.program for i in out.instruments]:\n",
    "                out.instruments.append(ct.Instrument(program=program, is_drum=False, name='reduction'))\n",
    "                instrument = out.instruments[-1]\n",
    "            else:\n",
    "                index = [i.program for i in out.instruments].index(program)\n",
    "                instrument = out.instruments[index]\n",
    "            instrument.notes.append(\n",
    "                ct.Note(\n",
    "                    start=int(current_beat + n[1]*480/12),\n",
    "                    end=int(current_beat + (n[1]+n[3]+1)*(480/12)),\n",
    "                    pitch=n[2] + 22,\n",
    "                    velocity=90\n",
    "                )\n",
    "            )\n",
    "\n",
    "    out.dump(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15daa2cb-2d0b-43e4-a211-e377a686030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 423 notes\n",
      "Changed 1 tokens\n"
     ]
    }
   ],
   "source": [
    "# get one sample\n",
    "X_test = X_test[0, :, :]\n",
    "y_test = y_test[0, 1:, :]\n",
    "\n",
    "token2mid(y_test.cpu().detach().numpy(), \"./test_ans.mid\")\n",
    "token2mid(X_test.cpu().detach().numpy(), \"./test_input.mid\")\n",
    "\n",
    "all_tokens = inference(X_test)\n",
    "token2mid(all_tokens, \"./test_gen.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
